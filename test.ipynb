{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.fx\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test my IDEA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MODEL(nn.Module):\n",
    "\tdef\t__init__(self, model_dict: dict[str, any], forward_order: list[str] = None):\n",
    "\t\tsuper().__init__()\n",
    "\t\tfor key, value in model_dict.items():\n",
    "\t\t\tself.add_module(key, value)\n",
    "\t\t\n",
    "\t\tself.forward_order = forward_order or list(model_dict.keys())\n",
    "\n",
    "\tdef\tforward(self, x):\n",
    "\t\tfor module_name in self.forward_order:\n",
    "\t\t\tmodule = getattr(self, module_name)\n",
    "\t\t\tx = module(x)\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "\t\"conv1\": nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1),\n",
    "\t\"relu1\": nn.ReLU(),\n",
    "\t\"pool1\": nn.MaxPool2d(2, 2),\n",
    "\t\"conv2\": nn.Conv2d(16, 32, 3, padding=1),\n",
    "\t\"relu2\": nn.ReLU(),\n",
    "\t\"pool2\": nn.MaxPool2d(2, 2),\n",
    "\t\"flatten\": nn.Flatten(),\n",
    "\t\"fc1\": nn.Linear(32 * 8 * 8, 128),\n",
    "\t\"relu3\": nn.ReLU(),\n",
    "\t\"fc2\": nn.Linear(128, 4),\n",
    "}\n",
    "\n",
    "forward_order = [\n",
    "\t\"conv1\", \"relu1\", \"pool1\",\n",
    "\t\"conv2\", \"relu2\", \"pool2\", \n",
    "\t\"flatten\", \"fc1\", \"relu3\", \"fc2\"\n",
    "]\n",
    "\n",
    "model = MODEL(model_dict, forward_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1700, -0.0129, -0.1568,  0.1397]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_input = torch.randn(1, 3, 32, 32)\n",
    "output = model.forward(dummy_input)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creator that can choose output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MODEL(nn.Module):\n",
    "    def __init__(self, model_dict: dict[str, any], computation_graph: list[tuple[str, list[str]]] = None):\n",
    "        super().__init__()\n",
    "        for key, value in model_dict.items():\n",
    "            self.add_module(key, value)\n",
    "\n",
    "        self.computation_graph = computation_graph\n",
    "        if computation_graph:\n",
    "            self.forward_order = self._build_forward_order(computation_graph)\n",
    "        else:\n",
    "            self.forward_order = list(model_dict.keys())\n",
    "        \n",
    "    def _build_forward_order(self, graph):\n",
    "        \"\"\"Build execution order from computation graph using topological sort\"\"\"\n",
    "        all_nodes = set(self._modules.keys())\n",
    "        adj = {node: [] for node in all_nodes}\n",
    "        indegree = {node: 0 for node in all_nodes}\n",
    "\n",
    "        for node, next_nodes in graph:\n",
    "            if node in all_nodes:\n",
    "                adj[node] = next_nodes\n",
    "                for next_node in next_nodes:\n",
    "                    if next_node in all_nodes:\n",
    "                        indegree[next_node] = indegree.get(next_node, 0) + 1\n",
    "        \n",
    "        queue = deque([node for node in all_nodes if indegree[node] == 0])\n",
    "        order = []\n",
    "\n",
    "        while queue:\n",
    "            node = queue.popleft()\n",
    "            order.append(node)\n",
    "            for neighbor in adj[node]:\n",
    "                if neighbor in all_nodes:\n",
    "                    indegree[neighbor] -= 1\n",
    "                    if indegree[neighbor] == 0:\n",
    "                        queue.append(neighbor)\n",
    "        \n",
    "        return order\n",
    "\n",
    "    def forward(self, x):\n",
    "        node_outputs = {}\n",
    "        \n",
    "        for module_name in self.forward_order:\n",
    "            module = getattr(self, module_name)\n",
    "            \n",
    "            # Find all input sources for this node\n",
    "            input_sources = []\n",
    "            for src_node, dest_nodes in self.computation_graph:\n",
    "                if module_name in dest_nodes:\n",
    "                    input_sources.append(src_node)\n",
    "            \n",
    "            # Handle input based on number of sources\n",
    "            if len(input_sources) == 0:\n",
    "                current_input = x\n",
    "            elif len(input_sources) == 1:\n",
    "                current_input = node_outputs[input_sources[0]]\n",
    "            else:\n",
    "                inputs = [node_outputs[src] for src in input_sources]\n",
    "                current_input = sum(inputs)\n",
    "            \n",
    "            output = module(current_input)\n",
    "            node_outputs[module_name] = output\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0152,  0.0952, -0.0076, -0.1873]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dict = {\n",
    "\t\"conv1\": nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1),\n",
    "\t\"relu1\": nn.ReLU(),\n",
    "\t\"pool1\": nn.MaxPool2d(2, 2),\n",
    "\t\"conv2\": nn.Conv2d(16, 32, 3, padding=1),\n",
    "\t\"relu2\": nn.ReLU(),\n",
    "\t\"pool2\": nn.MaxPool2d(2, 2),\n",
    "\t\"flatten\": nn.Flatten(),\n",
    "\t\"fc1\": nn.Linear(32 * 8 * 8, 128),\n",
    "\t\"relu3\": nn.ReLU(),\n",
    "\t\"fc2\": nn.Linear(128, 4),\n",
    "}\n",
    "\n",
    "computation_graph = [\n",
    "\t(\"conv1\", [\"relu1\"]),\n",
    "\t(\"relu1\", [\"pool1\"]),\n",
    "\t(\"pool1\", [\"conv2\"]),\n",
    "\t(\"conv2\", [\"relu2\"]),\n",
    "\t(\"relu2\", [\"pool2\"]),\n",
    "\t(\"pool2\", [\"flatten\"]),\n",
    "\t(\"flatten\", [\"fc1\"]),\n",
    "\t(\"fc1\", [\"relu3\"]),\n",
    "\t(\"relu3\", [\"fc2\"]),\n",
    "\t(\"fc2\", []),\n",
    "]\n",
    "\n",
    "model = MODEL(model_dict, computation_graph)\n",
    "dummy_input = torch.randn(1, 3, 32, 32)\n",
    "output = model.forward(dummy_input)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test Resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1000])\n",
      "tensor([[-4.3417e-01, -9.2019e-01,  5.0168e-01,  1.0639e+00,  5.6053e-01,\n",
      "          2.3193e-01,  1.6699e-01, -7.7604e-01,  8.8394e-01,  6.0368e-01,\n",
      "         -2.7266e-01,  2.3500e-01, -1.0095e-01, -7.7234e-01,  1.0674e+00,\n",
      "         -3.3842e-01, -7.8781e-01,  3.1908e-01, -3.2107e-01, -3.1551e-01,\n",
      "          8.1427e-01,  6.4469e-01,  4.6067e-01,  5.2217e-01, -8.2943e-01,\n",
      "          4.6688e-01,  7.0582e-01, -3.1540e-01, -9.8112e-02,  1.6774e-02,\n",
      "          5.4505e-01, -1.0040e-01,  3.4305e-01,  7.7029e-01, -5.7248e-01,\n",
      "          1.1978e-02, -3.1681e-01, -1.9202e-01, -6.1170e-02,  1.6212e+00,\n",
      "         -1.0591e+00, -5.0628e-01, -1.2541e+00,  2.1777e-01,  1.2237e+00,\n",
      "         -7.9195e-01,  1.6583e-01,  1.9182e-01, -5.7734e-01, -3.2444e-01,\n",
      "         -6.4151e-01,  5.9827e-01,  1.6499e-01, -1.3548e-01,  2.5947e-01,\n",
      "         -5.4122e-02,  3.6143e-01, -1.2056e-01, -3.4168e-01,  9.4372e-02,\n",
      "          7.3542e-01, -4.5201e-01, -4.3898e-02,  5.1047e-01,  3.5959e-01,\n",
      "         -1.9831e-01, -1.1153e-01,  2.4444e-01, -6.3406e-01,  6.0349e-01,\n",
      "         -4.5117e-01, -1.4176e-01,  9.1485e-01, -2.7952e-01,  3.6304e-01,\n",
      "         -7.8029e-01, -7.9664e-01,  4.9639e-01,  9.6255e-02,  9.7602e-01,\n",
      "         -2.1961e-01, -7.3452e-01,  1.2782e+00, -7.2242e-01,  6.7792e-01,\n",
      "         -3.7576e-01,  7.1458e-01,  4.9520e-01,  1.2602e-01,  1.1700e+00,\n",
      "         -4.7369e-01, -2.3922e-01, -5.3296e-01, -2.7440e-01,  9.5457e-02,\n",
      "         -8.4790e-01,  2.1125e-01, -7.9844e-01,  3.6021e-01,  5.0731e-01,\n",
      "          5.5557e-01, -8.7346e-01, -1.4310e-01,  2.9711e-01, -2.7212e-01,\n",
      "         -3.8971e-01,  9.2156e-01,  3.4910e-01, -6.9376e-01, -9.2204e-01,\n",
      "          9.5491e-02, -1.1048e-01,  1.8063e-01,  8.5458e-01,  5.5963e-01,\n",
      "          5.5564e-01, -3.9700e-01,  4.9475e-01, -6.3565e-01,  2.4234e-01,\n",
      "         -5.4946e-01,  4.4315e-01,  4.9746e-01, -2.1325e-01,  1.3446e-01,\n",
      "         -1.3054e-01,  1.4475e-01,  6.5122e-01,  4.5747e-01,  6.8113e-02,\n",
      "          8.2625e-01,  7.6980e-01,  2.7288e-01,  8.6510e-01, -1.8630e-02,\n",
      "          1.1992e+00, -9.3086e-01,  3.0195e-01,  7.7020e-01,  2.9239e-01,\n",
      "          6.3037e-01,  3.7638e-01, -7.8185e-01, -6.8175e-01,  7.5143e-01,\n",
      "         -3.2340e-01, -1.1838e+00,  4.7401e-01, -3.1160e-01, -8.7039e-01,\n",
      "          2.7339e-01, -1.3975e+00, -1.2519e+00, -1.9024e-01,  8.8563e-01,\n",
      "         -2.8117e-01, -5.1726e-01,  4.1171e-01, -1.9736e-01, -3.5023e-01,\n",
      "          4.6484e-01, -5.4046e-01, -3.2922e-01,  2.9279e-02,  6.6129e-01,\n",
      "          3.3137e-01, -6.1787e-01, -3.4249e-01, -4.6751e-01, -4.7908e-01,\n",
      "         -7.9335e-02,  1.0002e+00,  1.4196e-01, -6.4588e-01, -1.5219e-01,\n",
      "         -1.3117e-01,  7.9202e-01,  3.5372e-01, -7.6535e-02, -3.7137e-01,\n",
      "          3.4814e-01, -2.0643e-01, -8.1668e-01,  1.9460e-01,  6.9129e-01,\n",
      "          2.6697e-01, -3.6866e-01,  1.2706e+00, -1.5236e-01, -1.2366e+00,\n",
      "         -9.0733e-02,  3.6719e-03, -5.2185e-01,  5.6251e-01,  2.1406e-01,\n",
      "          9.2017e-02, -5.2665e-02,  1.2940e-01, -5.9811e-01, -4.8546e-01,\n",
      "         -3.1344e-02,  1.2770e-01,  3.0193e-02,  4.2559e-01, -6.8881e-01,\n",
      "          2.8821e-01,  6.5734e-01, -1.6649e-01, -7.5151e-01, -8.3857e-02,\n",
      "         -6.6158e-02, -2.1747e-01, -2.3146e-01, -1.5719e+00,  2.9755e-01,\n",
      "         -3.0215e-01, -1.4234e-01,  7.6001e-01, -3.5576e-01,  6.0514e-01,\n",
      "          2.6911e-02, -1.4587e-01,  4.0406e-01, -9.0855e-01, -7.7131e-01,\n",
      "         -1.3354e+00, -2.9828e-01,  1.0732e+00, -3.5716e-01,  3.7777e-01,\n",
      "          3.6066e-01,  1.8285e-01,  4.1734e-01,  2.1937e-01, -6.4425e-01,\n",
      "          1.2484e+00, -9.0393e-01,  2.1739e-01,  2.5340e-01,  7.3334e-01,\n",
      "         -3.2708e-01, -1.9450e+00,  9.9422e-01, -7.4341e-01, -1.0005e-01,\n",
      "          1.0494e-01,  3.6680e-01, -6.6172e-01,  1.6235e-01, -5.5821e-01,\n",
      "         -1.8548e-01,  7.4162e-02,  6.7492e-01,  7.0120e-01, -7.5127e-01,\n",
      "          7.3205e-01, -5.3181e-02,  3.3208e-01,  3.4430e-01,  1.2091e-01,\n",
      "         -1.2057e-01,  3.4313e-02, -1.1090e-01,  5.5922e-01,  2.1613e-01,\n",
      "          2.8285e-02, -2.9776e-02,  3.2730e-01, -2.5802e-02,  4.0225e-02,\n",
      "         -5.8112e-01,  7.8572e-03, -8.7942e-02,  4.6683e-01, -1.0080e-01,\n",
      "          4.1520e-01, -6.0172e-01, -6.6215e-01, -9.1056e-01,  7.4470e-01,\n",
      "         -2.4294e-01, -1.6703e-01,  6.3116e-01,  1.4908e-01, -1.0502e-01,\n",
      "          4.2883e-02, -3.6017e-01, -7.1340e-01, -1.0460e-01, -8.6936e-02,\n",
      "         -6.6489e-01, -5.9111e-01,  1.2922e+00, -2.3153e-01,  6.4568e-01,\n",
      "         -2.0398e-01, -1.5270e+00, -7.3301e-01, -7.3383e-01,  3.5463e-01,\n",
      "          8.6559e-01, -5.1843e-01,  5.7505e-02,  9.1611e-02, -2.6308e-01,\n",
      "         -2.3780e-01, -2.8350e-01, -5.8381e-01, -9.3528e-01,  5.5950e-01,\n",
      "          2.9618e-03,  4.5967e-01,  6.1450e-01, -3.4783e-02, -8.7226e-02,\n",
      "         -1.8063e+00, -9.7610e-01,  8.9208e-02, -3.7569e-01,  1.8297e-01,\n",
      "         -7.4434e-01, -2.5054e-01, -3.1771e-01,  2.7145e-01,  3.4848e-01,\n",
      "         -8.0014e-02,  1.9989e-01,  2.7360e-01,  6.5195e-01,  3.9976e-01,\n",
      "          2.5547e-01,  7.0883e-01,  8.0601e-01, -1.0324e+00, -2.6365e-01,\n",
      "         -4.8614e-01, -1.1851e-01,  6.9340e-01,  1.4522e+00, -4.0435e-01,\n",
      "         -5.5140e-01, -5.7657e-01,  9.8487e-01,  2.5737e-02, -1.8516e-01,\n",
      "          4.2241e-01, -4.7070e-01, -1.1210e+00,  2.1692e-01,  2.3733e-01,\n",
      "         -1.1121e+00,  1.5427e-01,  5.4238e-01, -1.1096e+00,  2.3881e-01,\n",
      "          5.6978e-01, -6.5254e-01,  1.2869e+00, -2.1694e-01,  6.2730e-01,\n",
      "          2.6693e-01, -7.2833e-01, -4.0540e-01, -5.9424e-01, -2.5512e-02,\n",
      "          3.4216e-01, -2.0018e-01,  1.6221e-01,  1.7219e+00,  1.4058e-01,\n",
      "         -5.1830e-01, -4.2225e-01, -7.5994e-01, -1.1478e-01, -1.3276e+00,\n",
      "         -7.0228e-01,  2.0347e-01, -9.9406e-01,  3.0688e-01,  3.0190e-02,\n",
      "          6.5763e-01,  3.1648e-01, -6.0654e-01,  1.4273e-01,  5.0284e-01,\n",
      "         -1.3713e-01,  5.6373e-01,  1.6444e-01, -1.9709e-01,  4.8653e-02,\n",
      "         -6.6104e-01, -2.4469e-01,  1.0195e+00, -2.1876e-02, -5.1411e-01,\n",
      "         -1.6732e-02,  5.9006e-01, -3.0294e-01, -3.7378e-01, -6.3707e-01,\n",
      "         -8.8172e-01, -6.8571e-01, -4.2702e-01, -2.4647e-01, -2.3951e-01,\n",
      "         -4.7123e-01,  1.2786e+00,  2.3889e-01,  4.4363e-01, -4.1024e-01,\n",
      "         -2.2352e-01,  5.2567e-01, -6.3235e-01,  6.6681e-02,  8.6439e-01,\n",
      "         -4.8200e-01, -6.1471e-02, -1.1955e-01,  1.8213e-01,  4.6434e-01,\n",
      "         -6.8566e-01,  1.1987e+00, -7.0046e-01, -1.1516e-04,  2.3341e-01,\n",
      "         -3.4014e-01,  8.3494e-01,  2.7250e-01, -1.7966e-01,  4.7982e-01,\n",
      "          4.3981e-01, -5.2186e-01, -1.5172e-01,  6.0346e-01, -1.1229e-01,\n",
      "          3.9918e-01,  4.6142e-01,  7.2959e-01,  2.6211e-01, -1.1050e+00,\n",
      "         -3.8646e-01,  3.5011e-01, -1.6715e-01, -2.6087e-01, -3.9209e-01,\n",
      "          3.6763e-01, -4.6167e-02, -1.9968e-01,  4.7726e-01, -9.2325e-01,\n",
      "          4.3331e-01, -5.8798e-01, -1.2479e+00,  3.7529e-01, -6.3339e-01,\n",
      "          8.5013e-01,  1.1917e+00, -1.0914e+00, -3.9519e-01,  2.6653e-01,\n",
      "          2.7508e-01, -4.3469e-01,  5.6153e-01, -4.8479e-01, -2.3622e-01,\n",
      "          1.2727e-02,  9.3071e-02,  7.1712e-02, -3.1483e-01, -9.1123e-01,\n",
      "         -3.0965e-04,  8.0939e-01, -5.7849e-01, -9.3141e-02, -9.5468e-01,\n",
      "          9.7045e-01, -4.8463e-01, -3.9905e-01, -2.5522e-01,  3.4446e-01,\n",
      "         -4.2198e-01, -3.9534e-01, -6.4089e-01, -5.1540e-01,  2.6770e-02,\n",
      "          1.7046e-01, -7.1127e-01,  3.0624e-01,  1.1432e-01,  2.4306e-01,\n",
      "          5.6076e-01,  1.6348e+00,  8.2897e-02, -5.4921e-01, -4.9508e-01,\n",
      "          3.4233e-02,  5.1676e-02,  2.2932e-01, -5.0082e-01,  1.0627e+00,\n",
      "          3.0381e-01,  3.7747e-01,  2.1824e-02,  2.7908e-01, -2.7100e-01,\n",
      "          7.6268e-01,  2.6234e-01, -2.9878e-01, -6.8396e-02,  4.9960e-01,\n",
      "          6.7339e-02, -1.3630e-01,  4.6001e-01,  2.5923e-01, -2.3580e-01,\n",
      "          2.0509e-01,  2.1094e-01, -7.6746e-01,  1.0222e+00,  9.8801e-02,\n",
      "         -2.0663e-01,  4.8891e-01,  8.5713e-01,  4.5752e-01, -7.4056e-01,\n",
      "          8.1617e-01,  1.4568e-01, -3.7009e-02, -4.4344e-01,  1.1763e-02,\n",
      "          1.7741e-01, -1.0063e+00, -7.6769e-01,  7.4080e-01, -8.6223e-01,\n",
      "          3.5628e-01, -1.0577e+00, -8.7700e-01, -5.0779e-01, -6.3099e-02,\n",
      "         -4.6516e-01, -3.1739e-01, -7.6609e-01,  9.6532e-01,  6.0061e-01,\n",
      "          5.6103e-01,  6.1242e-01, -6.8749e-01,  3.3942e-02,  1.7488e-02,\n",
      "         -5.0869e-02, -1.9230e-01,  1.6152e-01, -2.7059e-02,  6.1000e-02,\n",
      "          9.0867e-01,  3.8355e-01, -4.3712e-01,  3.6104e-01, -4.0635e-02,\n",
      "          3.8652e-02,  1.0661e+00,  1.4392e+00, -6.5444e-01,  9.3581e-01,\n",
      "         -3.3978e-02, -6.3324e-01, -2.8158e-01,  6.4428e-01,  4.6894e-01,\n",
      "         -1.0532e-01,  4.6539e-01,  7.3918e-01, -6.6992e-01, -4.9090e-01,\n",
      "          1.6823e-01,  3.1011e-01,  8.2786e-01, -4.6254e-01,  7.9353e-02,\n",
      "         -5.4201e-01,  1.1304e+00, -3.5899e-01, -4.1849e-01,  2.1084e-01,\n",
      "         -5.1269e-01, -4.5927e-01, -4.8671e-01, -6.2965e-01,  8.6345e-01,\n",
      "         -1.5005e-04, -1.0533e+00,  5.0933e-01,  8.1306e-01, -7.9986e-02,\n",
      "         -1.1665e+00, -8.0666e-01, -1.3305e-01,  5.0427e-01, -1.0464e+00,\n",
      "         -4.0942e-01,  8.0404e-01,  3.3706e-01,  7.7233e-01,  5.0114e-01,\n",
      "          1.0719e+00,  6.1681e-01,  4.2132e-01, -7.8218e-01,  7.7880e-01,\n",
      "          6.5876e-01, -4.1356e-01, -4.7676e-01,  1.8862e-01,  1.4619e+00,\n",
      "         -5.4152e-01, -9.6451e-02,  2.9332e-01,  7.1235e-01,  9.3770e-01,\n",
      "         -6.3281e-01,  3.5303e-01,  1.1107e+00,  7.7390e-01,  6.2134e-02,\n",
      "         -7.2295e-01,  3.9662e-01, -4.4750e-01,  3.1180e-01, -7.7519e-01,\n",
      "          5.1109e-01,  7.3595e-01, -2.4563e-01, -5.3385e-01, -9.9471e-02,\n",
      "         -2.4587e-01,  4.1878e-01,  5.1854e-01, -3.9709e-02, -7.7400e-01,\n",
      "         -9.2217e-01,  5.2461e-01,  9.8752e-01, -1.9188e-01, -7.3402e-01,\n",
      "          2.6937e-01, -4.0069e-01,  5.5586e-02, -1.4309e-01,  2.8216e-01,\n",
      "         -4.0152e-01,  4.7888e-01,  5.8189e-01,  2.1491e-01,  5.5279e-01,\n",
      "         -6.2957e-01, -3.7967e-01, -6.8564e-01,  7.0656e-02,  3.1300e-01,\n",
      "          2.9164e-01, -3.9619e-01, -4.9954e-01,  5.6341e-01,  4.9016e-01,\n",
      "          6.1845e-01, -5.2754e-02, -9.3662e-02, -9.5465e-01, -4.1852e-01,\n",
      "         -6.6363e-01,  2.3808e-01, -1.9009e-01, -5.8438e-01, -3.4974e-01,\n",
      "          1.3349e-01,  5.0900e-03,  3.1024e-01, -2.0537e-01, -3.6183e-01,\n",
      "          3.0264e-01, -1.9061e-01, -7.3233e-01,  6.7443e-01,  5.8635e-01,\n",
      "         -5.5019e-01, -2.2998e-01, -1.2693e-01,  1.1084e+00,  1.7620e-01,\n",
      "          5.8962e-01, -1.2383e+00, -9.3682e-01, -5.4579e-02, -5.2590e-01,\n",
      "         -4.2656e-01, -1.1595e+00,  4.2388e-01,  3.3636e-01, -8.9217e-02,\n",
      "         -6.2722e-01, -2.0933e-01,  6.9580e-01, -1.8985e-01,  8.3159e-01,\n",
      "         -2.8159e-01, -3.7724e-01, -9.5150e-01, -4.6574e-01,  5.8575e-01,\n",
      "          2.0648e-01,  5.2936e-01, -8.5734e-02, -7.7147e-01,  2.1311e-01,\n",
      "         -2.6753e-01,  4.3972e-01,  2.9664e-01, -4.3142e-01,  7.9305e-02,\n",
      "          1.0997e+00,  4.1810e-01,  3.8660e-01,  3.6898e-01,  8.1221e-01,\n",
      "         -6.1555e-01, -1.5075e-01,  2.2616e-01, -2.5977e-01, -8.6795e-01,\n",
      "         -5.1907e-01,  3.4256e-01,  8.6190e-02,  9.9873e-01,  9.7433e-01,\n",
      "         -3.0717e-01,  1.4196e+00,  6.3750e-01, -1.1289e+00,  4.2528e-01,\n",
      "         -1.7292e-02,  4.7109e-02, -8.3392e-02,  6.2791e-01,  5.1419e-01,\n",
      "         -1.8950e-01, -9.2622e-01, -1.2046e-01, -4.8063e-01,  5.2819e-01,\n",
      "          4.6988e-01,  6.6194e-01,  1.8077e+00, -8.6201e-01, -2.2210e-01,\n",
      "          3.7653e-01,  5.5865e-01, -9.5234e-02, -3.3584e-01, -1.3388e+00,\n",
      "          3.2851e-01,  9.4562e-02, -1.1776e-01,  1.9059e-01,  3.0330e-01,\n",
      "          1.8951e-01,  1.4250e+00, -1.5453e-01,  7.1645e-01, -2.8571e-01,\n",
      "         -8.3649e-01,  2.9340e-01,  1.5332e-02,  7.8059e-01,  3.5504e-01,\n",
      "          1.6333e-01,  8.2550e-03,  3.0978e-01,  1.8665e-01,  3.2163e-01,\n",
      "         -2.5727e-01, -4.2604e-01, -7.9197e-01, -5.8355e-02,  4.5517e-01,\n",
      "          9.1783e-02, -3.5551e-01,  1.0977e+00, -4.0470e-01,  2.5009e-01,\n",
      "         -5.2786e-01, -5.1664e-01, -1.5524e+00, -2.0297e-01, -2.4905e-01,\n",
      "          7.2211e-01, -9.8379e-01,  9.0589e-01,  6.8297e-01,  7.2872e-01,\n",
      "         -1.3602e+00, -3.5020e-01,  1.1339e+00,  5.4005e-01,  4.7976e-01,\n",
      "          9.6161e-01,  2.2998e-01, -3.6679e-01, -5.6748e-01, -7.2210e-01,\n",
      "         -6.9317e-01, -4.7737e-01,  5.6155e-02,  7.7913e-01, -4.7079e-01,\n",
      "         -1.1146e-01,  4.3032e-01,  3.6624e-01,  9.1595e-01, -1.4187e+00,\n",
      "          4.1995e-01, -1.0615e-01, -1.0732e+00,  1.1661e+00,  4.7522e-01,\n",
      "          2.4013e-01,  1.0993e+00,  1.2116e+00, -7.4976e-01, -9.4726e-01,\n",
      "          6.0562e-01, -2.5924e-01,  5.4695e-02,  2.5252e-01,  8.6749e-01,\n",
      "          3.8782e-01,  6.2378e-01, -5.7725e-01, -2.6153e-01,  1.0244e+00,\n",
      "          1.2704e+00,  2.0746e-01,  4.6731e-01,  1.6757e-02,  1.0866e+00,\n",
      "         -1.8529e-02,  5.1049e-01,  6.9480e-01,  7.6981e-01, -1.0495e+00,\n",
      "         -4.7787e-01,  1.2006e-01,  1.2063e-01, -2.5310e-01, -4.4881e-01,\n",
      "         -1.4810e-01, -1.7121e-01,  7.9375e-02,  2.7387e-01,  1.2486e-01,\n",
      "          1.6450e-01,  2.6626e-01, -1.3143e+00,  1.3192e-01,  7.1059e-02,\n",
      "         -2.2973e-01,  1.1472e-01, -1.1187e+00,  4.7249e-01, -1.5790e-01,\n",
      "          7.5350e-01,  1.3120e-01, -4.4090e-01, -1.5703e-01, -8.7664e-02,\n",
      "          3.5011e-01, -3.4179e-01,  5.6272e-01, -4.7742e-01, -4.7107e-01,\n",
      "          2.9227e-01,  2.4881e-01,  1.8534e-01, -1.4002e+00,  1.9749e-01,\n",
      "         -1.9862e-01,  4.3693e-01, -3.2475e-01, -3.9129e-01, -1.7762e-01,\n",
      "          2.9659e-01,  2.1616e-02,  3.9490e-01, -5.7408e-02, -6.3246e-01,\n",
      "         -1.0540e-01,  4.0084e-02,  1.5378e-01,  8.3595e-01,  7.3306e-01,\n",
      "         -3.7069e-01, -7.8567e-01,  5.2302e-01,  7.1621e-01,  4.7669e-01,\n",
      "         -1.2427e-01,  4.6212e-01, -9.8247e-01,  3.7086e-01, -2.8305e-01,\n",
      "         -2.2639e-01, -6.4621e-01, -5.1718e-01, -8.2136e-01,  4.9940e-01,\n",
      "          3.8006e-01, -5.8742e-01,  1.0320e+00, -1.1331e+00,  1.0715e+00,\n",
      "         -1.3684e+00,  2.0199e-01,  1.4038e-01, -3.8357e-01,  1.9721e-01,\n",
      "         -3.8765e-01, -7.3865e-01,  3.4580e-01, -3.8352e-01, -8.9448e-01,\n",
      "         -6.8489e-01,  9.0224e-01, -2.1900e-02,  6.4331e-01,  3.9004e-01,\n",
      "         -8.2082e-01, -4.7994e-01,  2.9781e-02, -3.5092e-02,  5.2259e-01,\n",
      "          1.0204e+00,  6.2443e-02,  3.9948e-01,  5.4834e-01,  5.8256e-01,\n",
      "          5.3079e-01,  1.3595e-01, -1.4717e-01,  1.0002e+00,  9.0509e-02,\n",
      "          1.5352e+00,  4.5773e-01,  8.7950e-01, -8.0727e-02,  1.2326e+00,\n",
      "          1.0205e+00, -3.5430e-01,  2.5315e-01, -6.9304e-01, -1.2792e-02,\n",
      "         -4.4820e-01, -2.2070e-01, -2.7469e-01, -9.3227e-01, -2.2045e-01,\n",
      "          5.1549e-01, -6.0008e-02,  1.2371e-01,  2.2315e-01, -4.5078e-01,\n",
      "         -1.4921e-01,  8.4153e-01,  1.5595e-01,  4.7679e-01, -4.7465e-02,\n",
      "         -2.5825e-01, -3.3817e-01, -9.8987e-02, -2.6413e-01,  5.5187e-01,\n",
      "         -3.9900e-01, -6.3727e-01, -3.4092e-01, -1.7996e-01,  5.9073e-01,\n",
      "          3.9961e-01,  3.5590e-01,  1.1860e+00,  6.0762e-01,  7.8288e-01,\n",
      "          1.2880e+00, -4.7061e-01,  1.0630e-01,  2.9765e-01, -9.3726e-02,\n",
      "          1.0824e-01, -4.6556e-01,  3.6382e-01,  1.2251e-01,  4.8019e-01]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mycnn_model_dict = {\n",
    "    # Block 0\n",
    "    \"conv0\": nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "    \"bn0\": nn.BatchNorm2d(64),\n",
    "    \"relu0\": nn.ReLU(),\n",
    "    \"maxpool0\": nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "    # Block 1\n",
    "    \"conv1\": nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "    \"bn1\": nn.BatchNorm2d(128),\n",
    "    \"relu1\": nn.ReLU(),\n",
    "    \"maxpool1\": nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "    # Block 2\n",
    "    \"conv2\": nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "    \"bn2\": nn.BatchNorm2d(256),\n",
    "    \"relu2\": nn.ReLU(),\n",
    "\n",
    "    \"conv3\": nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "    \"bn3\": nn.BatchNorm2d(256),\n",
    "    \"relu3\": nn.ReLU(),\n",
    "\n",
    "    # Block 3\n",
    "    \"conv4\": nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "    \"bn4\": nn.BatchNorm2d(512),\n",
    "    \"relu4\": nn.ReLU(),\n",
    "\n",
    "    \"conv5\": nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "    \"bn5\": nn.BatchNorm2d(512),\n",
    "    \"relu5\": nn.ReLU(),\n",
    "\n",
    "    \"conv6\": nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "    \"bn6\": nn.BatchNorm2d(512),\n",
    "    \"relu6\": nn.ReLU(),\n",
    "    \"maxpool6\": nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "    # Block 4\n",
    "    \"conv7\": nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "    \"bn7\": nn.BatchNorm2d(512),\n",
    "    \"relu7\": nn.ReLU(),\n",
    "\n",
    "    \"conv8\": nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "    \"bn8\": nn.BatchNorm2d(512),\n",
    "    \"relu8\": nn.ReLU(),\n",
    "\n",
    "    \"conv9\": nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "    \"bn9\": nn.BatchNorm2d(512),\n",
    "    \"relu9\": nn.ReLU(),\n",
    "    \"maxpool9\": nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "    # Block 5\n",
    "    \"conv10\": nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
    "    \"bn10\": nn.BatchNorm2d(256),\n",
    "    \"relu10\": nn.ReLU(),\n",
    "\n",
    "    \"conv11\": nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "    \"bn11\": nn.BatchNorm2d(256),\n",
    "    \"relu11\": nn.ReLU(),\n",
    "\n",
    "    \"conv12\": nn.Conv2d(256, 64, kernel_size=3, padding=1),\n",
    "    \"bn12\": nn.BatchNorm2d(64),\n",
    "    \"relu12\": nn.ReLU(),\n",
    "\n",
    "    \"conv13\": nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "    \"bn13\": nn.BatchNorm2d(64),\n",
    "    \"relu13\": nn.ReLU(),\n",
    "    \"maxpool13\": nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "    # Final classifier head\n",
    "    \"avgpool\": nn.AdaptiveAvgPool2d((1, 1)),\n",
    "    \"flatten\": nn.Flatten(),\n",
    "    \"fc\": nn.Linear(64, 1000),\n",
    "}\n",
    "\n",
    "mycnn_computation_graph = [\n",
    "    (\"conv0\", [\"bn0\"]),\n",
    "    (\"bn0\", [\"relu0\"]),\n",
    "    (\"relu0\", [\"maxpool0\"]),\n",
    "    (\"maxpool0\", [\"conv1\"]),\n",
    "\n",
    "    (\"conv1\", [\"bn1\"]),\n",
    "    (\"bn1\", [\"relu1\"]),\n",
    "    (\"relu1\", [\"maxpool1\"]),\n",
    "    (\"maxpool1\", [\"conv2\"]),\n",
    "\n",
    "    (\"conv2\", [\"bn2\"]),\n",
    "    (\"bn2\", [\"relu2\"]),\n",
    "    (\"relu2\", [\"conv3\"]),\n",
    "\n",
    "    (\"conv3\", [\"bn3\"]),\n",
    "    (\"bn3\", [\"relu3\"]),\n",
    "    (\"relu3\", [\"conv4\"]),\n",
    "\n",
    "    (\"conv4\", [\"bn4\"]),\n",
    "    (\"bn4\", [\"relu4\"]),\n",
    "    (\"relu4\", [\"conv5\"]),\n",
    "\n",
    "    (\"conv5\", [\"bn5\"]),\n",
    "    (\"bn5\", [\"relu5\"]),\n",
    "    (\"relu5\", [\"conv6\"]),\n",
    "\n",
    "    (\"conv6\", [\"bn6\"]),\n",
    "    (\"bn6\", [\"relu6\"]),\n",
    "    (\"relu6\", [\"maxpool6\"]),\n",
    "    (\"maxpool6\", [\"conv7\"]),\n",
    "\n",
    "    (\"conv7\", [\"bn7\"]),\n",
    "    (\"bn7\", [\"relu7\"]),\n",
    "    (\"relu7\", [\"conv8\"]),\n",
    "\n",
    "    (\"conv8\", [\"bn8\"]),\n",
    "    (\"bn8\", [\"relu8\"]),\n",
    "    (\"relu8\", [\"conv9\"]),\n",
    "\n",
    "    (\"conv9\", [\"bn9\"]),\n",
    "    (\"bn9\", [\"relu9\"]),\n",
    "    (\"relu9\", [\"maxpool9\"]),\n",
    "    (\"maxpool9\", [\"conv10\"]),\n",
    "\n",
    "    (\"conv10\", [\"bn10\"]),\n",
    "    (\"bn10\", [\"relu10\"]),\n",
    "    (\"relu10\", [\"conv11\"]),\n",
    "\n",
    "    (\"conv11\", [\"bn11\"]),\n",
    "    (\"bn11\", [\"relu11\"]),\n",
    "    (\"relu11\", [\"conv12\"]),\n",
    "\n",
    "    (\"conv12\", [\"bn12\"]),\n",
    "    (\"bn12\", [\"relu12\"]),\n",
    "    (\"relu12\", [\"conv13\"]),\n",
    "\n",
    "    (\"conv13\", [\"bn13\"]),\n",
    "    (\"bn13\", [\"relu13\"]),\n",
    "    (\"relu13\", [\"maxpool13\"]),\n",
    "\n",
    "    (\"maxpool13\", [\"avgpool\"]),\n",
    "    (\"avgpool\", [\"flatten\"]),\n",
    "    (\"flatten\", [\"fc\"]),\n",
    "    (\"fc\", []),\n",
    "]\n",
    "\n",
    "mycnn_model = MODEL(mycnn_model_dict, mycnn_computation_graph)\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "output = mycnn_model(dummy_input)\n",
    "\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Extractor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
